"""Search endpoints."""

from typing import List, Literal, Dict, Any, Optional
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

from app.schemas import ProfileSearchResult
from app.embeddings import get_embedding_service
from app.azure_search import get_azure_search_service
from app.cache import get_cache_service
from app.firebase_db import get_firebase_service

router = APIRouter(prefix="/search", tags=["search"])


class SearchRequest(BaseModel):
    """Request model for semantic search."""
    
    query: str = Field(..., min_length=1, description="Search query")
    limit: int = Field(10, ge=1, le=100, description="Max results")
    score_threshold: float = Field(0.3, ge=0, le=1, description="Minimum similarity score")
    mode: Literal["offers", "needs", "both"] = Field("offers", description="Which vector to search")


@router.post("", response_model=List[ProfileSearchResult])
def search_profiles(request: SearchRequest):
    """
    Semantic search for profiles with optional Redis caching.

    Uses Azure OpenAI embeddings to find profiles whose skills semantically match
    the search query. Results are cached for 1 hour to improve performance.

    Performance:
        - Cache Hit: ~5ms (16x faster)
        - Cache Miss: ~80ms (normal Azure AI Search)

    Example:
        Query: "teach me guitar and music"
        Returns: Profiles of people who can teach guitar, music theory, etc.
    """
    cache_service = get_cache_service()
    embedding_service = get_embedding_service()
    search_service = get_azure_search_service()
    
    # Try cache first
    cache_key = cache_service._generate_key(
        "search",
        {
            "query": request.query,
            "limit": request.limit,
            "threshold": request.score_threshold,
            "mode": request.mode,
        }
    )
    
    cached = cache_service.get(cache_key)
    if cached:
        print(f"✅ Cache HIT: '{request.query}' (mode={request.mode})")
        return [ProfileSearchResult(**result) for result in cached]
    
    print(f"❌ Cache MISS: '{request.query}' (mode={request.mode})")
    
    # Generate query embedding
    query_vec = embedding_service.encode(request.query)
    
    # Search by mode
    mode = request.mode
    if mode == "offers":
        results = search_service.search_offers(
            query_vec=query_vec,
            limit=request.limit,
            score_threshold=request.score_threshold,
        )
        # Cache the results
        cache_service.set(cache_key, results, ttl=3600)
        return [ProfileSearchResult(**result) for result in results]
    if mode == "needs":
        results = search_service.search_needs(
            query_vec=query_vec,
            limit=request.limit,
            score_threshold=request.score_threshold,
        )
        # Cache the results
        cache_service.set(cache_key, results, ttl=3600)
        return [ProfileSearchResult(**result) for result in results]

    # mode == "both": combine offers and needs; pick the higher score per uid
    offer_results = search_service.search_offers(
        query_vec=query_vec,
        limit=request.limit,
        score_threshold=request.score_threshold,
    )
    need_results = search_service.search_needs(
        query_vec=query_vec,
        limit=request.limit,
        score_threshold=request.score_threshold,
    )
    
    combined_by_uid = {}
    for item in offer_results + need_results:
        uid = item.get("uid") or item.get("username")
        if uid is None:
            # Fallback to pushing without dedupe if no uid present
            combined_by_uid[item.get("username")] = item
            continue
        prev = combined_by_uid.get(uid)
        if prev is None or item.get("score", 0) > prev.get("score", 0):
            combined_by_uid[uid] = item
    
    combined_list = list(combined_by_uid.values())
    # Sort by score desc and cap to limit
    combined_list.sort(key=lambda x: x.get("score", 0), reverse=True)
    combined_list = combined_list[: request.limit]
    
    # Cache the results
    cache_service.set(cache_key, combined_list, ttl=3600)
    
    return [ProfileSearchResult(**result) for result in combined_list]


class SkillRecommendationRequest(BaseModel):
    """Request for skill recommendations."""
    
    current_skills: str = Field(..., min_length=1, description="User's current skills or interests")
    limit: int = Field(5, ge=1, le=20, description="Number of recommendations")


class SkillRecommendation(BaseModel):
    """A recommended skill."""
    
    skill: str
    score: float
    reason: str


@router.post("/recommend-skills", response_model=List[SkillRecommendation])
def recommend_skills(request: SkillRecommendationRequest):
    """
    Recommend complementary skills based on user's current skills.
    
    Analyzes what skills are commonly learned together by finding profiles
    with similar skills and extracting their other skills.
    
    Example:
        Input: "Python programming"
        Output: ["SQL databases", "Docker", "Git version control", ...]
    """
    cache_service = get_cache_service()
    embedding_service = get_embedding_service()
    search_service = get_azure_search_service()

    # Try cache first
    cache_key = cache_service._generate_key(
        "skill_recommend",
        {"skills": request.current_skills, "limit": request.limit}
    )
    
    cached = cache_service.get(cache_key)
    if cached:
        print(f"✅ Cache HIT: Skill recommendations for '{request.current_skills}'")
        return [SkillRecommendation(**rec) for rec in cached]
    
    print(f"❌ Cache MISS: Generating skill recommendations for '{request.current_skills}'")
    
    # Encode the current skills
    query_vec = embedding_service.encode(request.current_skills)
    
    # Search for people with similar skills (both offers and needs)
    similar_offers = search_service.search_offers(
        query_vec=query_vec,
        limit=20,
        score_threshold=0.4,
    )

    similar_needs = search_service.search_needs(
        query_vec=query_vec,
        limit=20,
        score_threshold=0.4,
    )
    
    # Extract and rank complementary skills
    skill_frequencies: Dict[str, Dict[str, Any]] = {}
    
    # Process offers (what people can teach)
    for profile in similar_offers:
        offers = profile.get("skills_to_offer", "")
        if offers and offers.strip():
            # Simple extraction: split by common delimiters
            skills = [s.strip() for s in offers.replace(",", ".").split(".") if s.strip()]
            for skill in skills[:5]:  # Limit to first 5 skills per profile
                if skill and len(skill) > 10:  # Only meaningful skills
                    if skill not in skill_frequencies:
                        skill_frequencies[skill] = {"count": 0, "total_score": 0.0}
                    skill_frequencies[skill]["count"] += 1
                    skill_frequencies[skill]["total_score"] += profile.get("score", 0.5)
    
    # Process needs (what people want to learn - indicates trending skills)
    for profile in similar_needs:
        needs = profile.get("services_needed", "")
        if needs and needs.strip():
            skills = [s.strip() for s in needs.replace(",", ".").split(".") if s.strip()]
            for skill in skills[:5]:
                if skill and len(skill) > 10:
                    if skill not in skill_frequencies:
                        skill_frequencies[skill] = {"count": 0, "total_score": 0.0}
                    skill_frequencies[skill]["count"] += 1
                    skill_frequencies[skill]["total_score"] += profile.get("score", 0.5) * 0.8  # Weight needs slightly lower
    
    # Rank by frequency and relevance
    recommendations = []
    for skill, data in skill_frequencies.items():
        avg_score = data["total_score"] / max(data["count"], 1)
        combined_score = (data["count"] * 0.3) + (avg_score * 0.7)  # Balance frequency and relevance
        
        reason = f"Common among {data['count']} similar profiles"
        recommendations.append({
            "skill": skill,
            "score": round(combined_score, 3),
            "reason": reason
        })
    
    # Sort by combined score and take top N
    recommendations.sort(key=lambda x: x["score"], reverse=True)
    recommendations = recommendations[:request.limit]
    
    # Cache for 2 hours
    cache_service.set(cache_key, recommendations, ttl=7200)
    
    return [SkillRecommendation(**rec) for rec in recommendations]


def _skills_to_text(skills):
    """Convert skills array or string to text for embeddings."""
    if not skills:
        return None
    if isinstance(skills, str):
        return skills
    if isinstance(skills, list):
        parts = []
        for s in skills:
            if isinstance(s, dict):
                name = s.get('name') or s.get('title', '')
                level = s.get('level') or s.get('difficulty', '')
                if name:
                    parts.append(f"{name} ({level})" if level else name)
            elif isinstance(s, str):
                parts.append(s)
        return ', '.join(parts) if parts else None
    return None


class ReindexUserRequest(BaseModel):
    """Request to reindex a single user."""
    uid: str = Field(..., description="User ID to reindex")


class ReindexResponse(BaseModel):
    """Response from reindex operation."""
    success: bool
    message: str
    skills_indexed: Optional[str] = None


@router.post("/reindex-user", response_model=ReindexResponse)
def reindex_user(request: ReindexUserRequest):
    """
    Reindex a single user's skills in Azure AI Search.
    
    This should be called after a user posts a new skill to update
    the search index with their latest skills.
    
    Args:
        request: Contains the user ID to reindex
        
    Returns:
        Success status and message
    """
    firebase_service = get_firebase_service()
    embedding_service = get_embedding_service()
    azure_search_service = get_azure_search_service()
    
    uid = request.uid
    
    try:
        # Get user profile
        profile = firebase_service.get_profile(uid)
        if not profile:
            raise HTTPException(status_code=404, detail=f"Profile not found for uid: {uid}")
        
        # Get skills from skills collection (single source of truth)
        user_skills = firebase_service.get_skills_by_user(uid)
        skills_to_offer = _skills_to_text(user_skills) if user_skills else None
        
        # Fallback to profile.skillsToOffer for backwards compat
        if not skills_to_offer:
            skills_to_offer = _skills_to_text(
                profile.get('skills_to_offer') or profile.get('skillsToOffer')
            )
        
        # Get services needed from profile
        services_needed = _skills_to_text(
            profile.get('services_needed') or profile.get('servicesNeeded')
        )
        
        # Use placeholder if missing
        skills_to_offer = skills_to_offer or "general help"
        services_needed = services_needed or "general services"
        
        # Generate embeddings
        offer_vec = embedding_service.encode(skills_to_offer)
        need_vec = embedding_service.encode(services_needed)
        
        # Prepare payload
        payload = {
            "uid": uid,
            "email": profile.get('email'),
            "display_name": profile.get('display_name') or profile.get('displayName') or profile.get('fullName'),
            "photo_url": profile.get('photo_url') or profile.get('photoUrl'),
            "full_name": profile.get('full_name') or profile.get('fullName'),
            "username": profile.get('username'),
            "bio": profile.get('bio'),
            "city": profile.get('city'),
            "timezone": profile.get('timezone'),
            "skills_to_offer": skills_to_offer,
            "services_needed": services_needed,
            "dm_open": profile.get('dm_open', profile.get('dmOpen', True)),
            "show_city": profile.get('show_city', profile.get('showCity', True)),
        }
        
        # Upsert to Azure AI Search
        azure_search_service.upsert_profile(
            username=uid,
            offer_vec=offer_vec,
            need_vec=need_vec,
            payload=payload,
        )
        
        return ReindexResponse(
            success=True,
            message=f"Successfully reindexed user {uid}",
            skills_indexed=skills_to_offer
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to reindex user: {str(e)}")

